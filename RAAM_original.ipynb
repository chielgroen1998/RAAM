{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBbyPd1f5--u"
      },
      "source": [
        "code fixen\n",
        "tunen (periods, indicators etc.) https://technical-analysis-library-in-python.readthedocs.io/en/latest/\n",
        "RSI code verbeteren\n",
        "correlation\n",
        "assets optimaliseren\n",
        "value\n",
        "• (C) Averag Relative Correlation Momentum: to determine\n",
        "a portfolio’s diversification component. Calculation: 4\n",
        "months relative average correlations on assets’\n",
        "daily returns.\n",
        "• Rank(C) Ranked Average Correlation Momentum: to\n",
        "rank the assets according to the monthly Average\n",
        "Relative Correlations values in descending order.\n",
        "* mean reversion RAAM?\n",
        "* na weging, optimizen gebaseerd op past returns per asset?\n",
        "*volume indicators?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd-BcFoNfsy5",
        "outputId": "48eb5a8f-b04b-4e72-b901-2eeecdf90694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.1)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4mX6mJae-H-H"
      },
      "outputs": [],
      "source": [
        "# #!pip install pandas==2.2.1\n",
        "\n",
        "# import yfinance as yf\n",
        "# import pandas as pd\n",
        "# from concurrent.futures import ThreadPoolExecutor\n",
        "# #from pandas.sparse.dtypes import SparseDtype\n",
        "\n",
        "\n",
        "# # Define the ticker symbols for the stocks as a list\n",
        "# ticker_symbols = [\n",
        "#     \"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"PEP\", \"AVGO\",\n",
        "#     \"COST\", \"CSCO\", \"ADBE\", \"NFLX\", \"TMUS\", \"TXN\", \"CMCSA\", \"QCOM\", \"INTC\", \"HON\",\n",
        "#     \"AMD\", \"AMGN\", \"INTU\", \"ISRG\", \"BKNG\", \"MDLZ\", \"ADI\", \"LRCX\", \"VRTX\", \"MU\",\n",
        "#     \"AMAT\", \"SBUX\", \"GILD\", \"MRNA\", \"ADP\", \"PANW\", \"FISV\", \"CSX\", \"REGN\", \"MNST\",\n",
        "#      \"KLAC\", \"MAR\", \"NXPI\", \"ORLY\", \"ADSK\", \"MCHP\", \"AEP\", \"KDP\", \"SNPS\",\n",
        "#     \"FTNT\", \"IDXX\", \"LULU\", \"EXC\", \"CTAS\", \"PAYX\", \"XEL\", \"PCAR\", \"ODFL\", \"VRSK\",\n",
        "#     \"WBA\", \"CDNS\", \"AZN\", \"DLTR\", \"EBAY\", \"BIIB\", \"ROST\", \"CRWD\", \"CHTR\",\n",
        "#     \"FAST\", \"PDD\", \"ANSS\", \"MRVL\", \"TEAM\", \"WDAY\", \"BKR\", \"DDOG\", \"ZS\", \"CEG\",\n",
        "#     \"KHC\", \"VRSN\", \"CTSH\", \"SWKS\", \"OKTA\", \"EA\", \"LCID\", \"BIDU\", \"ALGN\",\n",
        "#     \"MELI\", \"JD\", \"LI\", \"NTES\",  \"ASML\", \"DXCM\", \"CPRT\"\n",
        "# ]\n",
        "\n",
        "\n",
        "# #nasdaq_100_tickers = [\n",
        "#     # \"AAPL\", \"MSFT\",  \"GOOG\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"PEP\", \"AVGO\",\n",
        "#     # \"COST\", \"CSCO\", \"ADBE\", \"NFLX\", \"TMUS\", \"TXN\", \"CMCSA\", \"QCOM\", \"INTC\", \"HON\",\n",
        "#     # \"AMD\", \"AMGN\", \"INTU\", \"ISRG\", \"BKNG\", \"MDLZ\", \"ADI\", \"LRCX\", \"VRTX\", \"MU\",\n",
        "#     # \"AMAT\", \"SBUX\", \"GILD\", \"MRNA\", \"ADP\", \"PANW\", \"FISV\", \"CSX\", \"REGN\", \"MNST\",\n",
        "#     # \"ATVI\", \"KLAC\", \"MAR\", \"NXPI\", \"ORLY\", \"ADSK\", \"MCHP\", \"AEP\", \"KDP\", \"SNPS\",\n",
        "#     # \"FTNT\", \"IDXX\", \"LULU\", \"EXC\", \"CTAS\", \"PAYX\", \"XEL\", \"PCAR\", \"ODFL\", \"VRSK\",\n",
        "#     # \"WBA\", \"CDNS\", \"AZN\", \"DLTR\", \"EBAY\", \"BIIB\", \"ROST\", \"CRWD\", \"SGEN\", \"CHTR\",\n",
        "#     # \"FAST\", \"PDD\", \"ANSS\", \"MRVL\", \"TEAM\", \"WDAY\", \"BKR\", \"DDOG\", \"ZS\", \"CEG\",\n",
        "#     # \"KHC\", \"VRSN\", \"CTSH\", \"SWKS\", \"OKTA\", \"EA\", \"LCID\", \"BIDU\", \"ALGN\",\n",
        "#     # \"MELI\", \"JD\", \"LI\", \"ZS\", \"NTES\", \"BIDU\", \"ASML\", \"DXCM\", \"CPRT\", \"CRWD\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# #[ \"XLC\", \"XLY\", \"XLP\", \"XLE\",\"XLF\",\"XLV\",\"XLI\",\"XLB\",\"XLRE\", \"XLk\", \"XLU\", \"VOX\",\"VND\",\"FXY\",\"FXF\",\"GLD\",\"IEF\",\"SH\",\"TLT\", \"SHY\"]\n",
        "\n",
        "# #['VV','IJH','IJR','EFA','EEM','RWR','DBC','VAW','AGG','TIP','IGOV','SHY']\n",
        "# #['MSFT', 'AAPL', 'NVDA',  'AMZN', 'META', 'AVGO', 'TSLA', 'COST',  'AMD', 'GOOGL', 'GOOG']\n",
        "# ##['XLY','XLV', 'XLU', 'XLP', 'XLK', 'XLI', 'XLF','XLE', 'XLB', 'VOX', 'RWR', 'FXY', 'FXF', 'GLD', 'IEF' ,'SH', 'TLT', 'SHY']\n",
        "# #['NVDA', 'PLUG', 'BAH', 'SFM', 'VRTX', 'WCN', 'SYK', 'CCJ', 'HAL', 'BHP', 'CRM', 'ASM', 'III', 'JPM', 'CRWD', 'ISRG', 'TSLA', 'AMD', 'AVGO', 'LIN', 'INGR', 'REGN', 'WM', 'EW', 'BWXT', 'SLB', 'RIO', 'ALLE', 'BK', 'BAC', 'WFC', 'ABBN.SW', 'GBTC', 'ASML', 'LDOS', 'MRNA', 'RSG', 'MDT', 'ALB', 'TS', 'EFX', 'KO', 'GS', 'ESTC', '6861.T', 'MSFT', 'TSM', 'BE', 'DXCM', 'BKR', 'FCX', 'RTX', 'C', 'AXP', 'ZS', '6954.T', 'META', 'NTES', 'APD', 'ZBH', 'FTI', 'VALE', 'DT', 'AMZN', 'EA', 'INTC', 'SAP', 'ILMN', 'NOV', 'NEM', 'MAS', 'GOOG', 'TTWO', 'QCOM', 'CW', 'IFF', 'ALNY', 'SNN', 'SQM', 'USB', 'V', 'NET', '6273.T', 'GOOGL', 'TXN', 'HII', 'BCPC', 'BNTX', 'CHX', 'ICE', 'VRNS']\n",
        "\n",
        "# #[\"AAPL\", \"AMZN\", \"BABA\", \"BAC\", \"DB\", \"META\", \"MSFT\", \"NFLX\", \"NVDA\", \"PFE\", \"T\", \"TSLA\", \"V\", \"WMT\", \"ZM\", 'TQQQ', 'DOW', 'UPRO', 'GBTC', \"AFP.XC\",\"IBE.MC\", \"SHNY\"]\n",
        "\n",
        "# startdate= '2014-01-01'\n",
        "# enddate='2024-12-31'\n",
        "\n",
        "# # mom_p = 5 #5\n",
        "# # vol_p = 15 #10\n",
        "# # RSI_p = 20 #30\n",
        "# # ass_amount = 9 #9\n",
        "# # MA_p = 20 #10\n",
        "# # cor_p = 15 # 15\n",
        "\n",
        "mom_p = 20 #26\n",
        "vol_p = 35\n",
        "RSI_p = 40\n",
        "ass_amount = 6\n",
        "MA_p = 40\n",
        "cor_p = 40 # monthly\n",
        "\n",
        "# # Download the stock data for each ticker and keep only the 'Adj Close' column\n",
        "# data_to_concat = []\n",
        "# for ticker in ticker_symbols:\n",
        "#     data = yf.download(\n",
        "#         tickers=ticker,\n",
        "#         start= (startdate),\n",
        "#         end= (enddate),\n",
        "#         interval='1wk'  # Weekly interval\n",
        "#     )\n",
        "#     adj_close = data['Adj Close'].rename(ticker)  # Rename the series to the ticker symbol\n",
        "#     data_to_concat.append(adj_close)\n",
        "\n",
        "# # Concatenate the 'Adj Close' data for each ticker into a single DataFrame\n",
        "# combined_data = pd.concat(data_to_concat, axis=1)\n",
        "# combined_data.dropna()\n",
        "\n",
        "# combined_data.to_csv('combined_adj_close_data.csv')\n",
        "\n",
        "\n",
        "# combined_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZE1HzcGUixl"
      },
      "source": [
        "  [\n",
        "    \"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"PEP\", \"AVGO\",\n",
        "    \"COST\", \"CSCO\", \"ADBE\", \"NFLX\", \"TMUS\", \"TXN\", \"CMCSA\", \"QCOM\", \"INTC\", \"HON\",\n",
        "    \"AMD\", \"AMGN\", \"INTU\", \"ISRG\", \"BKNG\", \"MDLZ\", \"ADI\", \"LRCX\", \"VRTX\", \"MU\",\n",
        "    \"AMAT\", \"SBUX\", \"GILD\", \"MRNA\", \"ADP\", \"PANW\", \"FISV\", \"CSX\", \"REGN\", \"MNST\",\n",
        "    \"KLAC\", \"MAR\", \"NXPI\", \"ORLY\", \"ADSK\", \"MCHP\", \"AEP\", \"KDP\", \"SNPS\",\n",
        "    \"FTNT\", \"IDXX\", \"LULU\", \"EXC\", \"CTAS\", \"PAYX\", \"XEL\", \"PCAR\", \"ODFL\", \"VRSK\",\n",
        "    \"WBA\", \"CDNS\", \"AZN\", \"DLTR\", \"EBAY\", \"BIIB\", \"ROST\", \"CRWD\", \"CHTR\",\n",
        "    \"FAST\", \"PDD\", \"ANSS\", \"MRVL\", \"TEAM\", \"WDAY\", \"BKR\", \"DDOG\", \"ZS\", \"CEG\",\n",
        "    \"KHC\", \"VRSN\", \"CTSH\", \"SWKS\", \"OKTA\", \"EA\", \"LCID\", \"BIDU\", \"ALGN\",\n",
        "    \"MELI\", \"JD\", \"LI\", \"NTES\", \"ASML\", \"DXCM\", \"CPRT\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu40w1ZjwU28",
        "outputId": "10e2c6b2-662d-4683-8b92-c9156cf14cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting download of stock data...\n",
            "\n",
            "Downloading data for AAPL...\n",
            "AAPL: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed AAPL (full history)\n",
            "\n",
            "Downloading data for MSFT...\n",
            "MSFT: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed MSFT (full history)\n",
            "\n",
            "Downloading data for GOOG...\n",
            "GOOG: Got 1094 prices from 2004-08-16 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed GOOG (full history)\n",
            "\n",
            "Downloading data for AMZN...\n",
            "AMZN: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed AMZN (full history)\n",
            "\n",
            "Downloading data for NVDA...\n",
            "NVDA: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed NVDA (full history)\n",
            "\n",
            "Downloading data for META...\n",
            "META: Got 690 prices from 2012-05-14 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed META (full history)\n",
            "\n",
            "Downloading data for TSLA...\n",
            "TSLA: Got 788 prices from 2010-06-28 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed TSLA (full history)\n",
            "\n",
            "Downloading data for PEP...\n",
            "PEP: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed PEP (full history)\n",
            "\n",
            "Downloading data for AVGO...\n",
            "AVGO: Got 835 prices from 2009-08-03 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed AVGO (full history)\n",
            "\n",
            "Downloading data for COST...\n",
            "COST: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed COST (full history)\n",
            "\n",
            "Downloading data for CSCO...\n",
            "CSCO: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed CSCO (full history)\n",
            "\n",
            "Downloading data for ADBE...\n",
            "ADBE: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed ADBE (full history)\n",
            "\n",
            "Downloading data for NFLX...\n",
            "NFLX: Got 1211 prices from 2002-05-20 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed NFLX (full history)\n",
            "\n",
            "Downloading data for TMUS...\n",
            "TMUS: Got 955 prices from 2007-04-16 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed TMUS (full history)\n",
            "\n",
            "Downloading data for TXN...\n",
            "TXN: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed TXN (full history)\n",
            "\n",
            "Downloading data for CMCSA...\n",
            "CMCSA: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed CMCSA (full history)\n",
            "\n",
            "Downloading data for QCOM...\n",
            "QCOM: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed QCOM (full history)\n",
            "\n",
            "Downloading data for INTC...\n",
            "INTC: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed INTC (full history)\n",
            "\n",
            "Downloading data for HON...\n",
            "HON: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed HON (full history)\n",
            "\n",
            "Downloading data for AMD...\n",
            "AMD: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed AMD (full history)\n",
            "\n",
            "Downloading data for AMGN...\n",
            "AMGN: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed AMGN (full history)\n",
            "\n",
            "Downloading data for INTU...\n",
            "INTU: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed INTU (full history)\n",
            "\n",
            "Downloading data for ISRG...\n",
            "ISRG: Got 1312 prices from 2000-06-12 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed ISRG (full history)\n",
            "\n",
            "Downloading data for BKNG...\n",
            "BKNG: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed BKNG (full history)\n",
            "\n",
            "Downloading data for MDLZ...\n",
            "MDLZ: Got 1260 prices from 2001-06-11 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed MDLZ (full history)\n",
            "\n",
            "Downloading data for ADI...\n",
            "ADI: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed ADI (full history)\n",
            "\n",
            "Downloading data for LRCX...\n",
            "LRCX: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed LRCX (full history)\n",
            "\n",
            "Downloading data for VRTX...\n",
            "VRTX: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed VRTX (full history)\n",
            "\n",
            "Downloading data for MU...\n",
            "MU: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed MU (full history)\n",
            "\n",
            "Downloading data for AMAT...\n",
            "AMAT: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed AMAT (full history)\n",
            "\n",
            "Downloading data for SBUX...\n",
            "SBUX: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed SBUX (full history)\n",
            "\n",
            "Downloading data for GILD...\n",
            "GILD: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed GILD (full history)\n",
            "\n",
            "Downloading data for MRNA...\n",
            "MRNA: Got 348 prices from 2018-12-03 00:00:00-05:00 to 2025-07-28 00:00:00-04:00\n",
            "Skipping MRNA - insufficient history (starts from 2018-12-03 00:00:00-05:00)\n",
            "\n",
            "Downloading data for ADP...\n",
            "ADP: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed ADP (full history)\n",
            "\n",
            "Downloading data for PANW...\n",
            "PANW: Got 681 prices from 2012-07-16 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed PANW (full history)\n",
            "\n",
            "Downloading data for FISV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:$FISV: possibly delisted; no timezone found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No data available for FISV\n",
            "\n",
            "Downloading data for CSX...\n",
            "CSX: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed CSX (full history)\n",
            "\n",
            "Downloading data for REGN...\n",
            "REGN: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed REGN (full history)\n",
            "\n",
            "Downloading data for MNST...\n",
            "MNST: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed MNST (full history)\n",
            "\n",
            "Downloading data for KLAC...\n",
            "KLAC: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed KLAC (full history)\n",
            "\n",
            "Downloading data for MAR...\n",
            "MAR: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed MAR (full history)\n",
            "\n",
            "Downloading data for NXPI...\n",
            "NXPI: Got 783 prices from 2010-08-02 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed NXPI (full history)\n",
            "\n",
            "Downloading data for ORLY...\n",
            "ORLY: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed ORLY (full history)\n",
            "\n",
            "Downloading data for ADSK...\n",
            "ADSK: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed ADSK (full history)\n",
            "\n",
            "Downloading data for MCHP...\n",
            "MCHP: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed MCHP (full history)\n",
            "\n",
            "Downloading data for AEP...\n",
            "AEP: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n",
            "Successfully processed AEP (full history)\n",
            "\n",
            "Downloading data for KDP...\n",
            "KDP: Got 900 prices from 2008-05-05 00:00:00-04:00 to 2025-07-28 00:00:00-04:00\n",
            "Successfully processed KDP (full history)\n",
            "\n",
            "Downloading data for SNPS...\n",
            "SNPS: Got 1335 prices from 2000-01-01 00:00:00-05:00 to 2025-07-26 00:00:00-04:00\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Define the ticker symbols for the stocks\n",
        "ticker_symbols = [\n",
        "    \"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"PEP\", \"AVGO\",\n",
        "    \"COST\", \"CSCO\", \"ADBE\", \"NFLX\", \"TMUS\", \"TXN\", \"CMCSA\", \"QCOM\", \"INTC\", \"HON\",\n",
        "    \"AMD\", \"AMGN\", \"INTU\", \"ISRG\", \"BKNG\", \"MDLZ\", \"ADI\", \"LRCX\", \"VRTX\", \"MU\",\n",
        "    \"AMAT\", \"SBUX\", \"GILD\", \"MRNA\", \"ADP\", \"PANW\", \"FISV\", \"CSX\", \"REGN\", \"MNST\",\n",
        "    \"KLAC\", \"MAR\", \"NXPI\", \"ORLY\", \"ADSK\", \"MCHP\", \"AEP\", \"KDP\", \"SNPS\",\n",
        "    \"FTNT\", \"IDXX\", \"LULU\", \"EXC\", \"CTAS\", \"PAYX\", \"XEL\", \"PCAR\", \"ODFL\", \"VRSK\",\n",
        "    \"WBA\", \"CDNS\", \"AZN\", \"DLTR\", \"EBAY\", \"BIIB\", \"ROST\", \"CRWD\", \"CHTR\",\n",
        "    \"FAST\", \"PDD\", \"ANSS\", \"MRVL\", \"TEAM\", \"WDAY\", \"BKR\", \"DDOG\", \"ZS\", \"CEG\",\n",
        "    \"KHC\", \"VRSN\", \"CTSH\", \"SWKS\", \"OKTA\", \"EA\", \"LCID\", \"BIDU\", \"ALGN\",\n",
        "    \"MELI\", \"JD\", \"LI\", \"NTES\", \"ASML\", \"DXCM\", \"CPRT\"\n",
        "]\n",
        "\n",
        "# Parameters - all UTC timestamps\n",
        "startdate = '2000-01-01'\n",
        "enddate = '2025-12-31'\n",
        "cutoff_date = '2014-07-01'\n",
        "\n",
        "# Create reference timestamps with timezone\n",
        "START_TS = pd.Timestamp(startdate).tz_localize('UTC')\n",
        "END_TS = pd.Timestamp(enddate).tz_localize('UTC')\n",
        "CUTOFF_TS = pd.Timestamp(cutoff_date).tz_localize('UTC')\n",
        "\n",
        "def download_stock_data(ticker):\n",
        "    \"\"\"\n",
        "    Download stock data for a single ticker with improved error handling.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\nDownloading data for {ticker}...\")\n",
        "\n",
        "        # Create a Ticker object\n",
        "        stock = yf.Ticker(ticker)\n",
        "\n",
        "        # Download the historical data\n",
        "        data = stock.history(\n",
        "            start=startdate,\n",
        "            end=enddate,\n",
        "            interval='1wk',\n",
        "            auto_adjust=True  # This ensures we get adjusted prices\n",
        "        )\n",
        "\n",
        "        if data.empty:\n",
        "            print(f\"No data available for {ticker}\")\n",
        "            return None\n",
        "\n",
        "        # Extract the closing prices\n",
        "        prices = data['Close']  # Use 'Close' instead of 'Adj Close' since auto_adjust=True\n",
        "\n",
        "        # Verify we have actual price data\n",
        "        if len(prices) == 0:\n",
        "            print(f\"No price data for {ticker}\")\n",
        "            return None\n",
        "\n",
        "        print(f\"{ticker}: Got {len(prices)} prices from {prices.index[0]} to {prices.index[-1]}\")\n",
        "\n",
        "        # Add a small delay to avoid rate limiting\n",
        "        time.sleep(1)  # Increased delay to be more conservative\n",
        "\n",
        "        return prices\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {ticker}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    print(\"Starting download of stock data...\")\n",
        "\n",
        "    # Download and store the data\n",
        "    all_data = {}\n",
        "    successful_downloads = 0\n",
        "    failed_downloads = 0\n",
        "    long_history_tickers = []\n",
        "\n",
        "    # Convert cutoff date to timezone-aware pandas timestamp\n",
        "    cutoff = pd.Timestamp(cutoff_date, tz='UTC')\n",
        "\n",
        "    # First pass: Download all data and identify stocks with sufficient history\n",
        "    for ticker in ticker_symbols:\n",
        "        series = download_stock_data(ticker)\n",
        "        if series is not None and not series.empty:\n",
        "            # Ensure index is timezone aware\n",
        "            if series.index.tz is None:\n",
        "                series.index = series.index.tz_localize('UTC')\n",
        "            # Check if the stock has data from before our cutoff date\n",
        "            if series.index[0] <= CUTOFF_TS:\n",
        "                long_history_tickers.append(ticker)\n",
        "                all_data[ticker] = series\n",
        "                successful_downloads += 1\n",
        "                print(f\"Successfully processed {ticker} (full history)\")\n",
        "            else:\n",
        "                print(f\"Skipping {ticker} - insufficient history (starts from {series.index[0]})\")\n",
        "                failed_downloads += 1\n",
        "        else:\n",
        "            failed_downloads += 1\n",
        "\n",
        "    # Create DataFrame and save results\n",
        "    if all_data:\n",
        "        # Convert to DataFrame\n",
        "        combined_data = pd.DataFrame(all_data)\n",
        "\n",
        "        # Save to CSV\n",
        "        combined_data.to_csv('combined_stock_data.csv')\n",
        "\n",
        "        print(\"\\nDownload Summary:\")\n",
        "        print(f\"Successfully downloaded: {successful_downloads} stocks\")\n",
        "        print(f\"Failed downloads: {failed_downloads} stocks\")\n",
        "        print(f\"Stocks with complete history from 2014: {len(long_history_tickers)}\")\n",
        "        print(f\"\\nShape of combined data: {combined_data.shape}\")\n",
        "        print(\"\\nDate range in data:\")\n",
        "        print(f\"Start: {combined_data.index[0]}\")\n",
        "        print(f\"End: {combined_data.index[-1]}\")\n",
        "        print(f\"\\nStocks in dataset: {len(combined_data.columns)}\")\n",
        "        print(\"\\nFirst few rows of the data:\")\n",
        "        print(combined_data.head())\n",
        "\n",
        "        # Print list of included stocks\n",
        "        print(\"\\nIncluded stocks with complete history:\")\n",
        "        print(', '.join(sorted(long_history_tickers)))\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo data was successfully downloaded!\")\n",
        "        print(f\"Attempted downloads: {len(ticker_symbols)}\")\n",
        "        print(f\"Failed downloads: {failed_downloads}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCe5j8lLFQAd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_stock_data(filepath='combined_stock_data.csv'):\n",
        "    \"\"\"\n",
        "    Load stock data with scientifically rigorous temporal indexing.\n",
        "\n",
        "    CRITICAL TEMPORAL PARAMETERS:\n",
        "    - UTC standardization\n",
        "    - Explicit timezone handling\n",
        "    - Mixed timezone resolution\n",
        "    \"\"\"\n",
        "    # Load with explicit UTC parsing to ensure temporal consistency\n",
        "    combined_data = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "\n",
        "    # CRITICAL: Convert index to UTC with proper timezone handling\n",
        "    combined_data.index = pd.to_datetime(combined_data.index, utc=True)\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "# Scientific usage:\n",
        "combined_data = load_stock_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7BLBRBkJMOA"
      },
      "outputs": [],
      "source": [
        "price_changes = combined_data.pct_change()\n",
        "volatility = price_changes.rolling(window= vol_p ).std()\n",
        "volatility_monthly = volatility.resample('M').last()\n",
        "ranked_volatility = volatility_monthly.rank(axis=1, method='first')\n",
        "\n",
        "ranked_volatility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQqOqHSr-k3w"
      },
      "outputs": [],
      "source": [
        "# Calculate percent change of prices\n",
        "price_changes = combined_data.pct_change()\n",
        "\n",
        "# Create a mask where the current price is lower than the lookback period\n",
        "mask1 = combined_data < combined_data.shift(mom_p)\n",
        "\n",
        "# Create a mask where the current price is below the moving average\n",
        "moving_average = combined_data.rolling(window=MA_p).mean()\n",
        "mask2 = combined_data < moving_average\n",
        "\n",
        "# Apply both masks to the price changes\n",
        "price_changes = price_changes.where(~(mask1 | mask2))\n",
        "\n",
        "# Calculate momentum rolling forward over a period of 16\n",
        "momentum = price_changes.rolling(window=mom_p).apply(lambda x: (x + 1).prod() - 1)\n",
        "\n",
        "momentum_monthly = momentum.resample('M').last()\n",
        "\n",
        "# Rank momentum in descending order\n",
        "ranked_momentum = momentum_monthly.rank(axis=1, method='first', ascending=False)\n",
        "\n",
        "\n",
        "\n",
        "ranked_momentum.to_csv('ranked_momentum.csv')\n",
        "\n",
        "ranked_momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQTN7Na-KFcW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate percentual change for each ticker\n",
        "pct_change_df = combined_data.pct_change()\n",
        "\n",
        "# Initialize an empty list to store correlation mean for each date entry\n",
        "correlation_means = []\n",
        "\n",
        "# Iterate through each row\n",
        "for index, row in pct_change_df.iterrows():\n",
        "    # Compute correlation for each ticker with other tickers and take the mean\n",
        "    other_tickers = [ticker for ticker in pct_change_df.columns if ticker != index]\n",
        "    correlation_mean = row.corr(pct_change_df[other_tickers].mean(axis=1))\n",
        "    correlation_means.append(correlation_mean)\n",
        "\n",
        "\n",
        "# Add the correlation means as a new column to the DataFrame\n",
        "pct_change_df['Correlation_Mean'] = correlation_means\n",
        "pct_change_df = pct_change_df.drop('Correlation_Mean', axis=1)\n",
        "\n",
        "# Resample the dataframe back every month\n",
        "resampled_df = pct_change_df.resample('M').mean()\n",
        "\n",
        "# Calculate the mean value of 16 months back\n",
        "rolling_mean_df = resampled_df.rolling(window=cor_p).mean()\n",
        "\n",
        "# Roll forward the rolling mean values\n",
        "rolling_mean_df = rolling_mean_df.shift(-1)\n",
        "\n",
        "# Drop the last row since it will be NaN after the shift\n",
        "rolling_mean_df = rolling_mean_df.iloc[:-1]\n",
        "\n",
        "# Create a new DataFrame to hold the rankings\n",
        "rankings_df = rolling_mean_df\n",
        "\n",
        "# Rank the tickers based on their correlation\n",
        "ranked_correlation = rolling_mean_df.rank(axis=1, method='first')\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "ranked_correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgFj2IluZXXg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Ensure the index is a datetime object\n",
        "combined_data.index = pd.to_datetime(combined_data.index)\n",
        "\n",
        "# Initialize an empty DataFrame to store the RSI values\n",
        "rsi_values = pd.DataFrame(index=combined_data.index)\n",
        "\n",
        "# Calculate the RSI for each stock\n",
        "for stock in combined_data.columns:\n",
        "    # Calculate the price changes for the current stock\n",
        "    stock_changes = combined_data[stock].pct_change()\n",
        "\n",
        "    # Calculate the gain and loss\n",
        "    gain = stock_changes.clip(lower=0)\n",
        "    loss = -stock_changes.clip(upper=0)\n",
        "\n",
        "    # Calculate the average gain and loss\n",
        "    avg_gain = gain.ewm(com=RSI_p, adjust=False).mean()  # Exponential moving average\n",
        "    avg_loss = loss.ewm(com=RSI_p, adjust=False).mean()  # Exponential moving average\n",
        "\n",
        "    # Calculate the RSI\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # Add the RSI values to the DataFrame\n",
        "    rsi_values[stock] = rsi\n",
        "\n",
        "# Resample the RSI values to monthly data\n",
        "rsi_monthly = rsi_values.resample('M').last()\n",
        "\n",
        "# Rank the RSI\n",
        "ranked_rsi = rsi_monthly.rank(axis=1, method='first', ascending=False)\n",
        "\n",
        "# Print the ranked RSI\n",
        "ranked_rsi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL7x1NSVditR"
      },
      "outputs": [],
      "source": [
        "# Define a separate set of weights for ranking\n",
        "ranking_weights = pd.Series({\n",
        "    'Momentum Score': 0,\n",
        "    'RSI Score': 1,\n",
        "    'Volatility Score': 0,\n",
        "    'Correlation Score': 0\n",
        "})\n",
        "\n",
        "\n",
        "# Calculate weighted scores\n",
        "weighted_momentum = ranked_momentum * ranking_weights['Momentum Score']\n",
        "weighted_rsi = ranked_rsi * ranking_weights['RSI Score']\n",
        "weighted_volatility = ranked_volatility * ranking_weights['Volatility Score']\n",
        "weighted_correlation = ranked_correlation * ranking_weights['Correlation Score']\n",
        "\n",
        "# Calculate the cumulative score with weights\n",
        "cumulative_score = weighted_momentum + weighted_rsi + weighted_volatility + weighted_correlation\n",
        "\n",
        "cumulative_score.to_csv('cumscore.csv', index=True)\n",
        "\n",
        "cumulative_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8YPLIdmZc1G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_portfolio_selections(cumulative_score, ass_amount):\n",
        "    \"\"\"\n",
        "    Process portfolio selections based on cumulative scores and create a date-ticker mapping.\n",
        "\n",
        "    Parameters:\n",
        "    cumulative_score (pd.DataFrame): DataFrame with cumulative scores\n",
        "    ass_amount (int): Number of assets to select\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with Date and Ticker columns for selected assets\n",
        "    \"\"\"\n",
        "    # Get the smallest n scores for each date\n",
        "    ranked_df = cumulative_score.apply(lambda x: x.nsmallest(ass_amount), axis=1)\n",
        "\n",
        "    # Create a boolean mask for valid selections (not null values)\n",
        "    ranked_mask = ranked_df.notna()\n",
        "\n",
        "    # Create DataFrame with ticker names where mask is True\n",
        "    result_df = pd.DataFrame(index=ranked_df.index, columns=ranked_df.columns)\n",
        "    for column in ranked_df.columns:\n",
        "        result_df[column] = ranked_mask[column].map({True: column, False: 0})\n",
        "\n",
        "    # Convert index to datetime if not already\n",
        "    result_df.index = pd.to_datetime(result_df.index)\n",
        "\n",
        "    # Shift dates forward by one month\n",
        "    result_df.index = result_df.index + pd.DateOffset(months=1)\n",
        "\n",
        "    # Create the output DataFrame more efficiently\n",
        "    dates = []\n",
        "    tickers = []\n",
        "\n",
        "    # Iterate through the DataFrame and collect non-zero entries\n",
        "    for date, row in result_df.iterrows():\n",
        "        valid_tickers = row[row != 0]\n",
        "        if not valid_tickers.empty:\n",
        "            dates.extend([date] * len(valid_tickers))\n",
        "            tickers.extend(valid_tickers.index)\n",
        "\n",
        "    # Create the final DataFrame\n",
        "    non_zero_df = pd.DataFrame({\n",
        "        'Date': dates,\n",
        "        'Ticker': tickers\n",
        "    })\n",
        "\n",
        "    # Ensure Date column is datetime\n",
        "    non_zero_df['Date'] = pd.to_datetime(non_zero_df['Date'])\n",
        "\n",
        "    return non_zero_df\n",
        "\n",
        "# Use the function\n",
        "portfolio_selections = process_portfolio_selections(cumulative_score, ass_amount)\n",
        "\n",
        "# Save to CSV\n",
        "portfolio_selections.to_csv('portfolio_selections.csv', index=False)\n",
        "\n",
        "# Print some information about the selections\n",
        "print(\"\\nPortfolio Selections Summary:\")\n",
        "print(f\"Total number of selections: {len(portfolio_selections)}\")\n",
        "print(f\"Date range: {portfolio_selections['Date'].min()} to {portfolio_selections['Date'].max()}\")\n",
        "print(f\"Number of unique tickers: {portfolio_selections['Ticker'].nunique()}\")\n",
        "print(\"\\nFirst few selections:\")\n",
        "print(portfolio_selections.head(50))\n",
        "print(portfolio_selections.tail(6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmjtlUDECiUF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "def calculate_stock_return(ticker: str, start_date: pd.Timestamp, end_date: pd.Timestamp) -> Dict[str, Any]:\n",
        "    \"\"\"Calculate return for a single stock with error handling\"\"\"\n",
        "    try:\n",
        "        stock = yf.download(\n",
        "            ticker,\n",
        "            start=start_date,\n",
        "            end=end_date + timedelta(days=1),\n",
        "            progress=False,\n",
        "            ignore_tz=True\n",
        "        )\n",
        "\n",
        "        if stock.empty or len(stock) < 2:\n",
        "            print(f\"Warning: Insufficient data for {ticker} between {start_date} and {end_date}\")\n",
        "            return None\n",
        "\n",
        "        first_price = float(stock['Close'].iloc[0].item())  # Updated float conversion\n",
        "        last_price = float(stock['Close'].iloc[-1].item())  # Updated float conversion\n",
        "        pct_change = ((last_price - first_price) / first_price) * 100\n",
        "\n",
        "        return {\n",
        "            'Start_Price': first_price,\n",
        "            'End_Price': last_price,\n",
        "            'Return_Pct': pct_change\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {ticker} for period {start_date} to {end_date}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def analyze_portfolio(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Analyze portfolio returns\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Group by date to get monthly portfolios\n",
        "    monthly_portfolios = df.groupby('Date')['Ticker'].apply(list).reset_index()\n",
        "\n",
        "    total_tickers = sum(len(tickers) for tickers in monthly_portfolios['Ticker'])\n",
        "    processed = 0\n",
        "\n",
        "    for _, row in monthly_portfolios.iterrows():\n",
        "        date = pd.to_datetime(row['Date'])\n",
        "        tickers = row['Ticker']\n",
        "\n",
        "        # Calculate start and end of month\n",
        "        start_date = date.replace(day=1)\n",
        "        end_date = (start_date + pd.offsets.MonthEnd(0))\n",
        "\n",
        "        for ticker in tickers:\n",
        "            return_data = calculate_stock_return(ticker, start_date, end_date)\n",
        "            processed += 1\n",
        "\n",
        "            if return_data is not None:\n",
        "                results.append({\n",
        "                    'Date': date,\n",
        "                    'Ticker': ticker,\n",
        "                    **return_data\n",
        "                })\n",
        "\n",
        "            # Print progress\n",
        "            if processed % 100 == 0:\n",
        "                print(f\"Processed {processed}/{total_tickers} stocks\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def format_summary(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Format the summary dataframe for better display\"\"\"\n",
        "    summary_df.index = summary_df.index.strftime('%Y-%m-%d')\n",
        "    return summary_df\n",
        "\n",
        "def print_analysis(returns_df: pd.DataFrame, summary_df: pd.DataFrame):\n",
        "    \"\"\"Print formatted analysis results\"\"\"\n",
        "    print(\"\\nPortfolio Analysis Summary:\")\n",
        "    print(f\"Total periods analyzed: {len(summary_df)}\")\n",
        "    print(f\"Total stocks analyzed: {len(returns_df)}\")\n",
        "\n",
        "    print(\"\\nFirst few rows of monthly summary:\")\n",
        "    print(format_summary(summary_df.head()))\n",
        "\n",
        "    print(\"\\nOverall Statistics:\")\n",
        "    print(f\"Average monthly return: {returns_df['Return_Pct'].mean():.2f}%\")\n",
        "    print(f\"Best monthly return: {returns_df['Return_Pct'].max():.2f}%\")\n",
        "    print(f\"Worst monthly return: {returns_df['Return_Pct'].min():.2f}%\")\n",
        "    print(f\"Return standard deviation: {returns_df['Return_Pct'].std():.2f}%\")\n",
        "\n",
        "    # Calculate annualized statistics\n",
        "    monthly_returns = returns_df.groupby('Date')['Return_Pct'].mean()\n",
        "    annualized_return = ((1 + monthly_returns/100).prod() ** (12/len(monthly_returns)) - 1) * 100\n",
        "    annualized_vol = monthly_returns.std() * np.sqrt(12)\n",
        "\n",
        "    print(f\"\\nAnnualized Statistics:\")\n",
        "    print(f\"Annualized Return: {annualized_return:.2f}%\")\n",
        "    print(f\"Annualized Volatility: {annualized_vol:.2f}%\")\n",
        "    print(f\"Sharpe Ratio (Rf=0): {(annualized_return/annualized_vol):.2f}\")\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Read your CSV data\n",
        "    portfolio_df = pd.read_csv(\"portfolio_selections.csv\")\n",
        "\n",
        "    # Process returns\n",
        "    print(\"Starting portfolio analysis...\")\n",
        "    returns_df = analyze_portfolio(portfolio_df)\n",
        "\n",
        "    # Generate summary\n",
        "    summary_df = returns_df.groupby('Date').agg({\n",
        "        'Return_Pct': [\n",
        "            ('Mean Return %', 'mean'),\n",
        "            ('Std Dev %', 'std'),\n",
        "            ('Min Return %', 'min'),\n",
        "            ('Max Return %', 'max'),\n",
        "            ('Count', 'count')\n",
        "        ]\n",
        "    }).round(2)\n",
        "\n",
        "    # Flatten column names\n",
        "    summary_df.columns = summary_df.columns.get_level_values(1)\n",
        "\n",
        "    # Save results\n",
        "    returns_df.to_csv('stock_returns_detailed.csv', index=False)\n",
        "    summary_df.to_csv('monthly_summary.csv')\n",
        "\n",
        "    # Print analysis\n",
        "    print_analysis(returns_df, summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DN5D6jhMOFDZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "def calculate_monthly_matrix(returns_series):\n",
        "    \"\"\"Convert monthly returns to a year x month matrix\"\"\"\n",
        "    df = pd.DataFrame({'returns': returns_series})\n",
        "    df['year'] = df.index.year\n",
        "    df['month'] = df.index.month\n",
        "    return df.pivot_table(index='year', columns='month', values='returns')\n",
        "\n",
        "def create_performance_charts(returns_df, benchmark_tickers=['SPY', 'QQQ']):\n",
        "    \"\"\"Create performance visualization suite\"\"\"\n",
        "    # Convert returns to decimal\n",
        "    returns_df['Return_Pct'] = returns_df['Return_Pct'] / 100\n",
        "\n",
        "    # Calculate portfolio performance\n",
        "    monthly_returns = returns_df.groupby('Date')['Return_Pct'].mean()\n",
        "    portfolio_cum_returns = (1 + monthly_returns).cumprod()\n",
        "\n",
        "    # Download benchmark data\n",
        "    benchmark_returns = {}\n",
        "    for ticker in benchmark_tickers:\n",
        "        print(f\"\\nDownloading {ticker} data...\")\n",
        "        benchmark_data = yf.download(ticker,\n",
        "                                   start=returns_df['Date'].min(),\n",
        "                                   end=returns_df['Date'].max(),\n",
        "                                   interval='1mo')\n",
        "        benchmark_returns[ticker] = benchmark_data['Close'].pct_change()\n",
        "\n",
        "    # Create figures\n",
        "    fig1 = make_subplots(rows=2, cols=1, shared_xaxes=True,\n",
        "                        subplot_titles=('Cumulative Returns (Log Scale)', 'Drawdowns'))\n",
        "\n",
        "    # Cumulative Returns Plot\n",
        "    fig1.add_trace(\n",
        "        go.Scatter(x=portfolio_cum_returns.index, y=portfolio_cum_returns,\n",
        "                  name='Portfolio', line=dict(color='blue')),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    for ticker, returns in benchmark_returns.items():\n",
        "        cum_returns = (1 + returns).cumprod()\n",
        "        fig1.add_trace(\n",
        "            go.Scatter(x=cum_returns.index, y=cum_returns,\n",
        "                      name=ticker, line=dict(dash='dash')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "    # Drawdowns Plot\n",
        "    drawdowns = (portfolio_cum_returns / portfolio_cum_returns.cummax() - 1)\n",
        "    fig1.add_trace(\n",
        "        go.Scatter(x=drawdowns.index, y=drawdowns,\n",
        "                  name='Portfolio Drawdowns', line=dict(color='red')),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    fig1.update_layout(height=800, title='Portfolio Performance Analysis')\n",
        "    fig1.update_yaxes(type=\"log\", row=1, col=1)\n",
        "\n",
        "    # Create monthly returns matrix for heatmap\n",
        "    monthly_matrix = calculate_monthly_matrix(monthly_returns)\n",
        "\n",
        "    # Create yearly comparison table\n",
        "    yearly_returns = monthly_returns.groupby(monthly_returns.index.year).apply(\n",
        "        lambda x: (1 + x).prod() - 1\n",
        "    )\n",
        "\n",
        "    yearly_comparison = pd.DataFrame({\n",
        "        'Portfolio': yearly_returns\n",
        "    })\n",
        "\n",
        "    for ticker, returns in benchmark_returns.items():\n",
        "        yearly_comparison[ticker] = returns.groupby(returns.index.year).apply(\n",
        "            lambda x: (1 + x).prod() - 1\n",
        "        )\n",
        "\n",
        "    # Create heatmap figure\n",
        "    fig2 = plt.figure(figsize=(15, 8))\n",
        "    sns.heatmap(monthly_matrix,\n",
        "                cmap='RdYlGn',\n",
        "                center=0,\n",
        "                annot=True,\n",
        "                fmt='.2%')\n",
        "    plt.title('Monthly Returns Heatmap')\n",
        "\n",
        "    # Calculate yearly statistics\n",
        "    yearly_stats = pd.DataFrame(index=yearly_returns.index)\n",
        "\n",
        "    for year in yearly_returns.index:\n",
        "        year_returns = monthly_returns[monthly_returns.index.year == year]\n",
        "\n",
        "        # Basic statistics\n",
        "        yearly_stats.loc[year, 'Return'] = yearly_returns[year]\n",
        "        yearly_stats.loc[year, 'Volatility'] = year_returns.std() * np.sqrt(12)\n",
        "        yearly_stats.loc[year, 'Sharpe'] = (yearly_returns[year] - 0.02) / (year_returns.std() * np.sqrt(12))\n",
        "\n",
        "        # Sortino Ratio\n",
        "        downside_returns = year_returns[year_returns < 0]\n",
        "        if len(downside_returns) > 0:\n",
        "            yearly_stats.loc[year, 'Sortino'] = (yearly_returns[year] - 0.02) / (downside_returns.std() * np.sqrt(12))\n",
        "        else:\n",
        "            yearly_stats.loc[year, 'Sortino'] = np.nan\n",
        "\n",
        "        # Maximum Drawdown\n",
        "        cum_returns = (1 + year_returns).cumprod()\n",
        "        yearly_stats.loc[year, 'Max Drawdown'] = (cum_returns / cum_returns.cummax() - 1).min()\n",
        "\n",
        "    return {\n",
        "        'performance_plot': fig1,\n",
        "        'heatmap': fig2,\n",
        "        'yearly_comparison': yearly_comparison,\n",
        "        'yearly_stats': yearly_stats\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Read your data\n",
        "    returns_df = pd.read_csv('stock_returns_detailed.csv')\n",
        "    returns_df['Date'] = pd.to_datetime(returns_df['Date'])\n",
        "\n",
        "    print(\"Creating performance charts...\")\n",
        "    analysis_results = create_performance_charts(returns_df)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nYearly Performance Comparison (%):\")\n",
        "    print(analysis_results['yearly_comparison'].round(4) * 100)\n",
        "\n",
        "    print(\"\\nYearly Statistics:\")\n",
        "    print(analysis_results['yearly_stats'].round(4))\n",
        "\n",
        "    # Save results\n",
        "    analysis_results['yearly_comparison'].to_csv('yearly_performance_comparison.csv')\n",
        "    analysis_results['yearly_stats'].to_csv('yearly_statistics.csv')\n",
        "\n",
        "    # Show plots\n",
        "    analysis_results['performance_plot'].show()\n",
        "    plt.show()  # Show the heatmap\n",
        "\n",
        "    # Additional analytics\n",
        "    monthly_returns = returns_df.groupby('Date')['Return_Pct'].mean()\n",
        "\n",
        "    print(\"\\nPortfolio Statistics:\")\n",
        "    print(f\"Total Return: {(((1 + monthly_returns).prod() - 1) * 100):.2f}%\")\n",
        "    print(f\"Annual Return: {(((1 + monthly_returns).prod() ** (12/len(monthly_returns)) - 1) * 100):.2f}%\")\n",
        "    print(f\"Monthly Volatility: {(monthly_returns.std() * 100):.2f}%\")\n",
        "    print(f\"Annual Volatility: {(monthly_returns.std() * np.sqrt(12) * 100):.2f}%\")\n",
        "    print(f\"Sharpe Ratio: {((monthly_returns.mean() - 0.02/12) / (monthly_returns.std()) * np.sqrt(12)):.2f}\")\n",
        "    print(f\"Max Drawdown: {(((1 + monthly_returns).cumprod() / (1 + monthly_returns).cumprod().cummax() - 1).min() * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tU4d4A7edfXF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class QuantitativeAnalysis:\n",
        "    def __init__(self, returns_df, risk_free_rate=0.02):\n",
        "        self.returns_df = returns_df.copy()\n",
        "        self.rf = risk_free_rate\n",
        "\n",
        "        # Standardize dates to UTC, then convert to naive datetime\n",
        "        self.returns_df['Date'] = pd.to_datetime(self.returns_df['Date']).dt.tz_localize(None)\n",
        "\n",
        "        # Calculate monthly returns\n",
        "        self.monthly_returns = self.returns_df.groupby('Date')['Return_Pct'].mean().astype(float) / 100\n",
        "        self.setup_benchmarks()\n",
        "\n",
        "    def setup_benchmarks(self):\n",
        "        \"\"\"Download and process benchmark data\"\"\"\n",
        "        self.benchmarks = {}\n",
        "        benchmark_tickers = ['SPY', 'QQQ']\n",
        "\n",
        "        for ticker in benchmark_tickers:\n",
        "            try:\n",
        "                # Download data\n",
        "                stock = yf.Ticker(ticker)\n",
        "                data = stock.history(\n",
        "                    start=self.returns_df['Date'].min(),\n",
        "                    end=self.returns_df['Date'].max(),\n",
        "                    interval='1mo'\n",
        "                )\n",
        "\n",
        "                # Convert index to naive datetime\n",
        "                data.index = pd.to_datetime(data.index).tz_localize(None)\n",
        "\n",
        "                # Calculate returns\n",
        "                returns = data['Close'].pct_change()\n",
        "                self.benchmarks[ticker] = returns\n",
        "\n",
        "                print(f\"Successfully downloaded {ticker} data\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download {ticker}: {str(e)}\")\n",
        "                self.benchmarks[ticker] = pd.Series(index=self.monthly_returns.index, dtype=float)\n",
        "\n",
        "    def calculate_beta(self, benchmark_ticker='SPY'):\n",
        "        try:\n",
        "            if benchmark_ticker not in self.benchmarks:\n",
        "                return np.nan\n",
        "\n",
        "            benchmark_returns = self.benchmarks[benchmark_ticker]\n",
        "\n",
        "            # Align dates\n",
        "            aligned_data = pd.DataFrame({\n",
        "                'portfolio': self.monthly_returns,\n",
        "                'benchmark': benchmark_returns\n",
        "            }).dropna()\n",
        "\n",
        "            cov = aligned_data['portfolio'].cov(aligned_data['benchmark'])\n",
        "            benchmark_var = aligned_data['benchmark'].var()\n",
        "\n",
        "            return cov / benchmark_var if benchmark_var != 0 else np.nan\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    def calculate_tracking_error(self, benchmark_ticker='SPY'):\n",
        "        try:\n",
        "            if benchmark_ticker not in self.benchmarks:\n",
        "                return np.nan\n",
        "\n",
        "            benchmark_returns = self.benchmarks[benchmark_ticker]\n",
        "            active_returns = self.monthly_returns - benchmark_returns\n",
        "            return active_returns.std() * np.sqrt(12)\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    def calculate_information_ratio(self, benchmark_ticker='SPY'):\n",
        "        try:\n",
        "            tracking_error = self.calculate_tracking_error(benchmark_ticker)\n",
        "            if tracking_error == 0 or np.isnan(tracking_error):\n",
        "                return np.nan\n",
        "\n",
        "            benchmark_returns = self.benchmarks[benchmark_ticker]\n",
        "            active_returns = self.monthly_returns - benchmark_returns\n",
        "            return active_returns.mean() / tracking_error * np.sqrt(12)\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "    def calculate_risk_metrics(self):\n",
        "        \"\"\"Calculate comprehensive risk metrics\"\"\"\n",
        "        returns = self.monthly_returns\n",
        "\n",
        "        # Basic metrics\n",
        "        cum_returns = (1 + returns).cumprod()\n",
        "        drawdowns = cum_returns / cum_returns.expanding().max() - 1\n",
        "        monthly_vol = returns.std()\n",
        "        annualized_vol = monthly_vol * np.sqrt(12)\n",
        "\n",
        "        metrics = {\n",
        "            'Total Return': (1 + returns).prod() - 1,\n",
        "            'CAGR': (1 + returns).prod() ** (12/len(returns)) - 1,\n",
        "            'Volatility (Annual)': annualized_vol,\n",
        "            'Sharpe Ratio': (returns.mean() - self.rf/12) / monthly_vol * np.sqrt(12),\n",
        "            'Max Drawdown': drawdowns.min(),\n",
        "            'Skewness': returns.skew(),\n",
        "            'Kurtosis': returns.kurtosis(),\n",
        "            'VaR (95%)': returns.quantile(0.05),\n",
        "            'CVaR (95%)': returns[returns <= returns.quantile(0.05)].mean(),\n",
        "            'Win Rate': (returns > 0).mean(),\n",
        "            'Best Month': returns.max(),\n",
        "            'Worst Month': returns.min(),\n",
        "            'Positive Months': (returns > 0).sum(),\n",
        "            'Negative Months': (returns < 0).sum(),\n",
        "            'Average Positive Month': returns[returns > 0].mean(),\n",
        "            'Average Negative Month': returns[returns < 0].mean(),\n",
        "        }\n",
        "\n",
        "        # Add benchmark-dependent metrics\n",
        "        if 'SPY' in self.benchmarks and not self.benchmarks['SPY'].empty:\n",
        "            metrics['Beta (vs S&P500)'] = self.calculate_beta()\n",
        "            metrics['Tracking Error (vs S&P500)'] = self.calculate_tracking_error()\n",
        "\n",
        "        return pd.Series(metrics)\n",
        "\n",
        "    def create_performance_dashboard(self):\n",
        "        \"\"\"Create main performance dashboard\"\"\"\n",
        "        returns = self.monthly_returns\n",
        "        cum_returns = (1 + returns).cumprod()\n",
        "\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Cumulative Returns (Log Scale)',\n",
        "                'Drawdowns',\n",
        "                'Return Distribution',\n",
        "                'Monthly Returns'\n",
        "            ),\n",
        "            vertical_spacing=0.15\n",
        "        )\n",
        "\n",
        "        # 1. Cumulative Returns\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=cum_returns.index, y=cum_returns,\n",
        "                      name='Portfolio', line=dict(color='blue')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Add benchmarks\n",
        "        for ticker, bench_returns in self.benchmarks.items():\n",
        "            if not bench_returns.empty:\n",
        "                bench_cum_returns = (1 + bench_returns).cumprod()\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(x=bench_cum_returns.index, y=bench_cum_returns,\n",
        "                              name=ticker, line=dict(dash='dash')),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "        # 2. Drawdowns\n",
        "        drawdowns = cum_returns / cum_returns.cummax() - 1\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=drawdowns.index, y=drawdowns,\n",
        "                      name='Drawdowns', line=dict(color='red')),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Return Distribution\n",
        "        fig.add_trace(\n",
        "            go.Histogram(x=returns, name='Monthly Returns',\n",
        "                        nbinsx=30, histnorm='probability'),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 4. Monthly Returns\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=returns.index, y=returns,\n",
        "                  name='Monthly Returns'),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            width=1200,\n",
        "            showlegend=True,\n",
        "            title_text=\"Portfolio Performance Dashboard\"\n",
        "        )\n",
        "        fig.update_yaxes(type=\"log\", row=1, col=1)\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_monthly_heatmap(self):\n",
        "        \"\"\"Create monthly returns heatmap\"\"\"\n",
        "        returns_df = pd.DataFrame({\n",
        "            'year': self.monthly_returns.index.year,\n",
        "            'month': self.monthly_returns.index.month,\n",
        "            'returns': self.monthly_returns.values\n",
        "        })\n",
        "\n",
        "        monthly_matrix = returns_df.pivot(\n",
        "            index='year',\n",
        "            columns='month',\n",
        "            values='returns'\n",
        "        )\n",
        "\n",
        "        fig = px.imshow(\n",
        "            monthly_matrix,\n",
        "            labels=dict(x=\"Month\", y=\"Year\", color=\"Returns\"),\n",
        "            x=[f\"Month {i}\" for i in range(1, 13)],\n",
        "            y=monthly_matrix.index,\n",
        "            aspect=\"auto\",\n",
        "            color_continuous_scale=\"RdYlGn\"\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=\"Monthly Returns Heatmap\",\n",
        "            height=600,\n",
        "            width=1000\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"Generate comprehensive analysis report\"\"\"\n",
        "        print(\"Calculating risk metrics...\")\n",
        "        metrics = self.calculate_risk_metrics()\n",
        "\n",
        "        print(\"Creating visualizations...\")\n",
        "        dashboard = self.create_performance_dashboard()\n",
        "        heatmap = self.create_monthly_heatmap()\n",
        "\n",
        "        # Save results\n",
        "        metrics.to_csv('risk_metrics.csv')\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\nKey Performance Metrics:\")\n",
        "        print(metrics.round(4))\n",
        "\n",
        "        # Display visualizations\n",
        "        dashboard.show()\n",
        "        heatmap.show()\n",
        "\n",
        "        return {\n",
        "            'metrics': metrics,\n",
        "            'dashboard': dashboard,\n",
        "            'heatmap': heatmap\n",
        "        }\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Read data\n",
        "    returns_df = pd.read_csv('stock_returns_detailed.csv')\n",
        "    returns_df['Date'] = pd.to_datetime(returns_df['Date'])\n",
        "\n",
        "    print(\"Initializing analysis...\")\n",
        "    analysis = QuantitativeAnalysis(returns_df)\n",
        "\n",
        "    print(\"Generating report...\")\n",
        "    results = analysis.generate_report()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}